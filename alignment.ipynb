{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Testing score aggregation for languages with no white spaces\n",
        "\n",
        "Problem: The explainability method produces one score per token. The scores need to be aligned with words. In languages with white space, this can be done by considering sequences divided by spaces as individual words. However, in languages where white space is not used or it has a different meaning (i.e. sentence break), a parser needs to be used:\n",
        "\n",
        "<style type=\"text/css\">\n",
        ".tg  {border-collapse:collapse;border-spacing:0;}\n",
        ".tg td{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;\n",
        "  overflow:hidden;padding:10px 5px;word-break:normal;}\n",
        ".tg th{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;\n",
        "  font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}\n",
        ".tg .tg-baqh{text-align:center;vertical-align:top}\n",
        ".tg .tg-0lax{text-align:left;vertical-align:top}\n",
        ".tg .tg-0pky{border-color:inherit;text-align:left;vertical-align:top}\n",
        "</style>\n",
        "<table class=\"tg\" style=\"undefined;table-layout: fixed; width: 460px\">\n",
        "<colgroup>\n",
        "<col style=\"width: 157.2px\">\n",
        "<col style=\"width: 43.2px\">\n",
        "<col style=\"width: 45.2px\">\n",
        "<col style=\"width: 59.2px\">\n",
        "<col style=\"width: 42.2px\">\n",
        "<col style=\"width: 51.2px\">\n",
        "<col style=\"width: 62.2px\">\n",
        "</colgroup>\n",
        "<thead>\n",
        "  <tr>\n",
        "    <th class=\"tg-0lax\"></th>\n",
        "    <th class=\"tg-baqh\" colspan=\"6\">tokens</th>\n",
        "  </tr>\n",
        "</thead>\n",
        "<tbody>\n",
        "  <tr>\n",
        "    <td class=\"tg-0pky\">model tokenization</td>\n",
        "    <td class=\"tg-0pky\">我</td>\n",
        "    <td class=\"tg-0pky\" colspan=\"2\">的名字</td>\n",
        "    <td class=\"tg-0pky\">是</td>\n",
        "    <td class=\"tg-0pky\">明</td>\n",
        "    <td class=\"tg-0pky\">珠</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td class=\"tg-0pky\">model scores</td>\n",
        "    <td class=\"tg-0pky\">0.2</td>\n",
        "    <td class=\"tg-0pky\" colspan=\"2\">0.14</td>\n",
        "    <td class=\"tg-0pky\">-0.1</td>\n",
        "    <td class=\"tg-0pky\">-0.05</td>\n",
        "    <td class=\"tg-0pky\">0.03</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td class=\"tg-0pky\">\"correct\" tokenization</td>\n",
        "    <td class=\"tg-0pky\">我</td>\n",
        "    <td class=\"tg-0pky\">的</td>\n",
        "    <td class=\"tg-0pky\">名字</td>\n",
        "    <td class=\"tg-0pky\">是</td>\n",
        "    <td class=\"tg-0pky\" colspan=\"2\">明珠</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td class=\"tg-0pky\">aggregated scores</td>\n",
        "    <td class=\"tg-0pky\">0.2</td>\n",
        "    <td class=\"tg-0pky\">0.14</td>\n",
        "    <td class=\"tg-0pky\">0.14</td>\n",
        "    <td class=\"tg-0pky\">-0.1</td>\n",
        "    <td class=\"tg-0pky\" colspan=\"2\">-0.01</td>\n",
        "  </tr>\n",
        "</tbody>\n",
        "</table>\n",
        "\n",
        "\n",
        "In this file the alignment is tested using 4 languages supported by spaCy which use white space as sentence divider and not between words: Simplified Chinese, Japanese, Korean, and Thai."
      ],
      "metadata": {
        "id": "1XZY7GCqzAHt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NyqLhAW8p8Ag",
        "outputId": "230e32dd-4098-4786-c20b-f311a47551da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-01-15 08:30:05.707942: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-01-15 08:30:05.708054: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-01-15 08:30:05.710277: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-01-15 08:30:08.332566: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Collecting zh-core-web-md==3.6.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/zh_core_web_md-3.6.0/zh_core_web_md-3.6.0-py3-none-any.whl (78.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.0/78.0 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.7.0,>=3.6.0 in /usr/local/lib/python3.10/dist-packages (from zh-core-web-md==3.6.0) (3.6.1)\n",
            "Requirement already satisfied: spacy-pkuseg<0.1.0,>=0.0.27 in /usr/local/lib/python3.10/dist-packages (from zh-core-web-md==3.6.0) (0.0.33)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->zh-core-web-md==3.6.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->zh-core-web-md==3.6.0) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->zh-core-web-md==3.6.0) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->zh-core-web-md==3.6.0) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->zh-core-web-md==3.6.0) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->zh-core-web-md==3.6.0) (8.1.12)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->zh-core-web-md==3.6.0) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->zh-core-web-md==3.6.0) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->zh-core-web-md==3.6.0) (2.0.10)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->zh-core-web-md==3.6.0) (0.9.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->zh-core-web-md==3.6.0) (0.10.3)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->zh-core-web-md==3.6.0) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->zh-core-web-md==3.6.0) (4.66.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->zh-core-web-md==3.6.0) (1.23.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->zh-core-web-md==3.6.0) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->zh-core-web-md==3.6.0) (1.10.13)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->zh-core-web-md==3.6.0) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->zh-core-web-md==3.6.0) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->zh-core-web-md==3.6.0) (23.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->zh-core-web-md==3.6.0) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->zh-core-web-md==3.6.0) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->zh-core-web-md==3.6.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->zh-core-web-md==3.6.0) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->zh-core-web-md==3.6.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->zh-core-web-md==3.6.0) (2023.11.17)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->zh-core-web-md==3.6.0) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->zh-core-web-md==3.6.0) (0.1.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.7.0,>=3.6.0->zh-core-web-md==3.6.0) (8.1.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.7.0,>=3.6.0->zh-core-web-md==3.6.0) (2.1.3)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('zh_core_web_md')\n",
            "2024-01-15 08:30:27.436609: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-01-15 08:30:27.436675: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-01-15 08:30:27.438067: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-01-15 08:30:29.157897: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Collecting ja-core-news-md==3.6.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/ja_core_news_md-3.6.0/ja_core_news_md-3.6.0-py3-none-any.whl (42.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.1/42.1 MB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.7.0,>=3.6.0 in /usr/local/lib/python3.10/dist-packages (from ja-core-news-md==3.6.0) (3.6.1)\n",
            "Requirement already satisfied: sudachipy!=0.6.1,>=0.5.2 in /usr/local/lib/python3.10/dist-packages (from ja-core-news-md==3.6.0) (0.6.8)\n",
            "Requirement already satisfied: sudachidict-core>=20211220 in /usr/local/lib/python3.10/dist-packages (from ja-core-news-md==3.6.0) (20230927)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->ja-core-news-md==3.6.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->ja-core-news-md==3.6.0) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->ja-core-news-md==3.6.0) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->ja-core-news-md==3.6.0) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->ja-core-news-md==3.6.0) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->ja-core-news-md==3.6.0) (8.1.12)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->ja-core-news-md==3.6.0) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->ja-core-news-md==3.6.0) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->ja-core-news-md==3.6.0) (2.0.10)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->ja-core-news-md==3.6.0) (0.9.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->ja-core-news-md==3.6.0) (0.10.3)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->ja-core-news-md==3.6.0) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->ja-core-news-md==3.6.0) (4.66.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->ja-core-news-md==3.6.0) (1.23.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->ja-core-news-md==3.6.0) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->ja-core-news-md==3.6.0) (1.10.13)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->ja-core-news-md==3.6.0) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->ja-core-news-md==3.6.0) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->ja-core-news-md==3.6.0) (23.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->ja-core-news-md==3.6.0) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->ja-core-news-md==3.6.0) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->ja-core-news-md==3.6.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->ja-core-news-md==3.6.0) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->ja-core-news-md==3.6.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->ja-core-news-md==3.6.0) (2023.11.17)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->ja-core-news-md==3.6.0) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->ja-core-news-md==3.6.0) (0.1.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.7.0,>=3.6.0->ja-core-news-md==3.6.0) (8.1.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.7.0,>=3.6.0->ja-core-news-md==3.6.0) (2.1.3)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('ja_core_news_md')\n",
            "2024-01-15 08:30:51.332955: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-01-15 08:30:51.333029: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-01-15 08:30:51.334485: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-01-15 08:30:53.154469: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Collecting ko-core-news-sm==3.6.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/ko_core_news_sm-3.6.0/ko_core_news_sm-3.6.0-py3-none-any.whl (14.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.7/14.7 MB\u001b[0m \u001b[31m28.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.7.0,>=3.6.0 in /usr/local/lib/python3.10/dist-packages (from ko-core-news-sm==3.6.0) (3.6.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->ko-core-news-sm==3.6.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->ko-core-news-sm==3.6.0) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->ko-core-news-sm==3.6.0) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->ko-core-news-sm==3.6.0) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->ko-core-news-sm==3.6.0) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->ko-core-news-sm==3.6.0) (8.1.12)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->ko-core-news-sm==3.6.0) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->ko-core-news-sm==3.6.0) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->ko-core-news-sm==3.6.0) (2.0.10)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->ko-core-news-sm==3.6.0) (0.9.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->ko-core-news-sm==3.6.0) (0.10.3)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->ko-core-news-sm==3.6.0) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->ko-core-news-sm==3.6.0) (4.66.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->ko-core-news-sm==3.6.0) (1.23.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->ko-core-news-sm==3.6.0) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->ko-core-news-sm==3.6.0) (1.10.13)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->ko-core-news-sm==3.6.0) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->ko-core-news-sm==3.6.0) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->ko-core-news-sm==3.6.0) (23.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->ko-core-news-sm==3.6.0) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->ko-core-news-sm==3.6.0) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->ko-core-news-sm==3.6.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->ko-core-news-sm==3.6.0) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->ko-core-news-sm==3.6.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->ko-core-news-sm==3.6.0) (2023.11.17)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->ko-core-news-sm==3.6.0) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->ko-core-news-sm==3.6.0) (0.1.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.7.0,>=3.6.0->ko-core-news-sm==3.6.0) (8.1.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.7.0,>=3.6.0->ko-core-news-sm==3.6.0) (2.1.3)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('ko_core_news_sm')\n",
            "Requirement already satisfied: spacy_thai in /usr/local/lib/python3.10/dist-packages (0.7.3)\n",
            "Requirement already satisfied: deplacy>=2.0.3 in /usr/local/lib/python3.10/dist-packages (from spacy_thai) (2.0.8)\n",
            "Requirement already satisfied: pythainlp>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from spacy_thai) (4.0.2)\n",
            "Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy_thai) (3.6.1)\n",
            "Requirement already satisfied: ufal.udpipe>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy_thai) (1.3.1.1)\n",
            "Requirement already satisfied: requests>=2.22.0 in /usr/local/lib/python3.10/dist-packages (from pythainlp>=2.3.2->spacy_thai) (2.31.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.2.2->spacy_thai) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.2.2->spacy_thai) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.2.2->spacy_thai) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.2.2->spacy_thai) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.2.2->spacy_thai) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.2.2->spacy_thai) (8.1.12)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.2.2->spacy_thai) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.2.2->spacy_thai) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.2.2->spacy_thai) (2.0.10)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.2.2->spacy_thai) (0.9.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.2.2->spacy_thai) (0.10.3)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.2.2->spacy_thai) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.2.2->spacy_thai) (4.66.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.2.2->spacy_thai) (1.23.5)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.2.2->spacy_thai) (1.10.13)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.2.2->spacy_thai) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy>=2.2.2->spacy_thai) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.2.2->spacy_thai) (23.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.2.2->spacy_thai) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=2.2.2->spacy_thai) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->pythainlp>=2.3.2->spacy_thai) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->pythainlp>=2.3.2->spacy_thai) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->pythainlp>=2.3.2->spacy_thai) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->pythainlp>=2.3.2->spacy_thai) (2023.11.17)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy>=2.2.2->spacy_thai) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy>=2.2.2->spacy_thai) (0.1.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy>=2.2.2->spacy_thai) (8.1.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy>=2.2.2->spacy_thai) (2.1.3)\n"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "!python -m spacy download zh_core_web_md\n",
        "!python -m spacy download ja_core_news_md\n",
        "!python -m spacy download ko_core_news_sm\n",
        "!pip install spacy_thai"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Chinese"
      ],
      "metadata": {
        "id": "qluhKwmuMxhZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example usage of spaCy"
      ],
      "metadata": {
        "id": "a3CvVHiFvdKF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(\"zh_core_web_md\")"
      ],
      "metadata": {
        "id": "vKQi8W3wqUhr"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from spacy.lang.zh.examples import sentences as zh_sentences\n",
        "\n",
        "doc = nlp(zh_sentences[0])\n",
        "print(doc.text)\n",
        "for token in doc:\n",
        "    print(token.text, token.pos_, token.dep_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0P4ouscCqNvs",
        "outputId": "8d769b79-192b-4b2b-f909-7f60ecbf31fe"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "作为语言而言，为世界使用人数最多的语言，目前世界有五分之一人口做为母语。\n",
            "作为 ADP case\n",
            "语言 NOUN nmod:prep\n",
            "而言 PART case\n",
            "， PUNCT punct\n",
            "为 ADP case\n",
            "世界 NOUN compound:nn\n",
            "使用 NOUN compound:nn\n",
            "人数 NOUN nsubj\n",
            "最多 VERB acl\n",
            "的 PART mark\n",
            "语言 NOUN ROOT\n",
            "， PUNCT punct\n",
            "目前 NOUN nmod:tmod\n",
            "世界 NOUN dep\n",
            "有 VERB conj\n",
            "五分之一 NUM dep\n",
            "人口 NOUN nsubj\n",
            "做为 VERB ccomp\n",
            "母语 NOUN dobj\n",
            "。 PUNCT punct\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example of tokenisation"
      ],
      "metadata": {
        "id": "Liwv6i7zvl2P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer"
      ],
      "metadata": {
        "id": "Ihx26x59rEhI"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"xlm-roberta-base\")"
      ],
      "metadata": {
        "id": "dEznLPlorY_L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b7c95ab-9ce7-4a3f-f956-f8392ce2e9bf"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"作为语言而言，为世界使用人数最多的语言，目前世界有五分之一人口做为母语。\"\n",
        "for i in tokenizer(text).input_ids:\n",
        "    print(tokenizer.decode(i))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VA0yL3cBre5k",
        "outputId": "f3b1e5a3-24a6-4112-e531-569b9cfb3ada"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<s>\n",
            "\n",
            "作为\n",
            "语言\n",
            "而言\n",
            ",\n",
            "为\n",
            "世界\n",
            "使用\n",
            "人数\n",
            "最多的\n",
            "语言\n",
            ",\n",
            "目前\n",
            "世界\n",
            "有\n",
            "五\n",
            "分\n",
            "之一\n",
            "人口\n",
            "做\n",
            "为\n",
            "母\n",
            "语\n",
            "。\n",
            "</s>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Adding scores (and transforming to the format explain.py uses)"
      ],
      "metadata": {
        "id": "OlKPgG0_vyNC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import itertools\n",
        "parsed_text = [i.text for i in nlp(zh_sentences[0])]\n",
        "tokenized_text = [tokenizer.decode(i) for i in tokenizer(zh_sentences[0]).input_ids]\n",
        "scores = [0.2+np.round(np.sin(0.8*i/np.pi), decimals=1) for i in range(len(tokenized_text))]"
      ],
      "metadata": {
        "id": "4bh_3elJrsH6"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Comparison:"
      ],
      "metadata": {
        "id": "xqMWCkbNv54S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i,j,k in itertools.zip_longest(scores, tokenized_text, parsed_text):\n",
        "    print(f'{i} \\t {j} \\t {k}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3XjjlYoQsiXT",
        "outputId": "3a8a4187-c765-4ae4-ddfc-92cda887e039"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.2 \t <s> \t 作为\n",
            "0.5 \t  \t 语言\n",
            "0.7 \t 作为 \t 而言\n",
            "0.8999999999999999 \t 语言 \t ，\n",
            "1.1 \t 而言 \t 为\n",
            "1.2 \t , \t 世界\n",
            "1.2 \t 为 \t 使用\n",
            "1.2 \t 世界 \t 人数\n",
            "1.1 \t 使用 \t 最多\n",
            "1.0 \t 人数 \t 的\n",
            "0.8 \t 最多的 \t 语言\n",
            "0.5 \t 语言 \t ，\n",
            "0.30000000000000004 \t , \t 目前\n",
            "0.0 \t 目前 \t 世界\n",
            "-0.2 \t 世界 \t 有\n",
            "-0.39999999999999997 \t 有 \t 五分之一\n",
            "-0.6000000000000001 \t 五 \t 人口\n",
            "-0.7 \t 分 \t 做为\n",
            "-0.8 \t 之一 \t 母语\n",
            "-0.8 \t 人口 \t 。\n",
            "-0.7 \t 做 \t None\n",
            "-0.6000000000000001 \t 为 \t None\n",
            "-0.39999999999999997 \t 母 \t None\n",
            "-0.2 \t 语 \t None\n",
            "0.0 \t 。 \t None\n",
            "0.30000000000000004 \t </s> \t None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Alignment function"
      ],
      "metadata": {
        "id": "9ZNMjOLKEtO6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "special_tokens = [\"<s>\", \"</s>\"]\n",
        "\n",
        "ASSERTION_MSG = lambda p,t,sp: '''\n",
        "    Given parsing of the sentence cannot be aligned.\n",
        "    Checking these might help:\n",
        "    - if string.isalpha() is applicable for your language\n",
        "    - the variable \"special_tokens\" contains the special tokens of your model,\n",
        "    - the two tokenizations are of the same sentence.\n",
        "    ''' + f'\\nThe sentences are \\n {p} \\n {t} \\nand the special tokens are {sp}.'\n",
        "\n",
        "def align(parsed, tokenized, scores):\n",
        "    \"\"\"\n",
        "    Function to align two different tokenizations of the same text.\n",
        "    E.g. for Chinese, the model tokenizer might tokenize\n",
        "    母语 (mother tongue) as \"母\" and \"语\".\n",
        "    -> if you have defined a better tokenized, give both tokenizations\n",
        "    to this function and it will align them and the scores\n",
        "    associated with the model tokenizer.\n",
        "    All punctuation is removed for this tokenization.\n",
        "    -> would be removed in the keyword extraction step anyway.\n",
        "    \"\"\"\n",
        "\n",
        "    parsed = [re.sub(r'[^\\D\\s]|[^\\w\\s]', '', p) for p in parsed if any(j.isalpha() for j in p)]\n",
        "    to_be_dropped = [i for i in range(len(tokenized)) if any(j.isalpha() for j in tokenized[i]) and tokenized[i] not in special_tokens]\n",
        "    tokenized = [re.sub(r'[^\\D\\s]|[^\\w\\s]', '',tokenized[i]) for i in to_be_dropped]\n",
        "    scores = [scores[i] for i in to_be_dropped]\n",
        "\n",
        "\n",
        "    assert \"\".join(parsed)==\"\".join(tokenized), ASSERTION_MSG(parsed, tokenized, special_tokens)\n",
        "\n",
        "    # align\n",
        "    # t_ind contains the current index we're at now in the tokenized (by model) sentence\n",
        "    t_ind=0\n",
        "    agg_scores = np.zeros(len(parsed))\n",
        "    # for each \"real tokenization\" (here \"parsed\")\n",
        "    for p_ind in range(len(parsed)):\n",
        "        sub_scores = []\n",
        "        for p in parsed[p_ind]:     # for each char in that tokenization\n",
        "            if p == tokenized[t_ind][0]:        # if theres a match\n",
        "                sub_scores.append(scores[t_ind])    # add the score for the character\n",
        "                if len(tokenized[t_ind])>1:         # remove the character for next calculation\n",
        "                    tokenized[t_ind] = tokenized[t_ind][1:]\n",
        "                else:\n",
        "                    tokenized[t_ind]=\"-\"        # not necessary but helped debug\n",
        "                    t_ind+=1                    # move to next index\n",
        "            else:\n",
        "                raise Exception(\"Alignment impossible for unforeseen reasons.\")       # despite assertion, something went wrong\n",
        "        agg_scores[p_ind] = np.mean(sub_scores)     # aggregate the scores TODO: method\n",
        "\n",
        "    return parsed, agg_scores.tolist()\n",
        "\n",
        "align(parsed_text, tokenized_text, scores)\n"
      ],
      "metadata": {
        "id": "J06Kh_6PsjcN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d231398-4c39-43c3-c892-fb1173913216"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['作为',\n",
              "  '语言',\n",
              "  '而言',\n",
              "  '为',\n",
              "  '世界',\n",
              "  '使用',\n",
              "  '人数',\n",
              "  '最多',\n",
              "  '的',\n",
              "  '语言',\n",
              "  '目前',\n",
              "  '世界',\n",
              "  '有',\n",
              "  '五分之一',\n",
              "  '人口',\n",
              "  '做为',\n",
              "  '母语'],\n",
              " [0.7,\n",
              "  0.8999999999999999,\n",
              "  1.1,\n",
              "  1.2,\n",
              "  1.2,\n",
              "  1.1,\n",
              "  1.0,\n",
              "  0.8,\n",
              "  0.8,\n",
              "  0.5,\n",
              "  0.0,\n",
              "  -0.2,\n",
              "  -0.39999999999999997,\n",
              "  -0.7250000000000001,\n",
              "  -0.8,\n",
              "  -0.65,\n",
              "  -0.3])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Japanese"
      ],
      "metadata": {
        "id": "hf1n1YBJ0tle"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from spacy.lang.ja.examples import sentences as ja_sentences\n",
        "\n",
        "nlp = spacy.load(\"ja_core_news_md\")\n",
        "doc = nlp(ja_sentences[0])\n",
        "print(doc.text)\n",
        "for token in doc:\n",
        "    print(token.text, token.pos_, token.dep_)"
      ],
      "metadata": {
        "id": "AlobhXjQ00MX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b3eedaa3-47a6-4bca-cae0-62c3116eaa2c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "アップルがイギリスの新興企業を１０億ドルで購入を検討\n",
            "アップル PROPN nsubj\n",
            "が ADP case\n",
            "イギリス PROPN nmod\n",
            "の ADP case\n",
            "新興 NOUN compound\n",
            "企業 NOUN obj\n",
            "を ADP case\n",
            "１０億 NUM punct\n",
            "ドル NOUN obl\n",
            "で ADP case\n",
            "購入 NOUN obj\n",
            "を ADP case\n",
            "検討 NOUN ROOT\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "parsed_text = [i.text for i in nlp(ja_sentences[0])]\n",
        "tokenized_text = [tokenizer.decode(i) for i in tokenizer(ja_sentences[0]).input_ids]\n",
        "scores = [0.2+np.round(np.sin(0.8*i/np.pi), decimals=1) for i in range(len(tokenized_text))]"
      ],
      "metadata": {
        "id": "f65R1qAN06hc"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i,j,k in itertools.zip_longest(scores, tokenized_text, parsed_text):\n",
        "    print(f'{i} \\t {j} \\t {k}')"
      ],
      "metadata": {
        "id": "24vvWyEC1QO-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db27ea09-30ed-4b0d-9262-bb83f5710a4f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.2 \t <s> \t アップル\n",
            "0.5 \t  \t が\n",
            "0.7 \t アップ \t イギリス\n",
            "0.8999999999999999 \t ル \t の\n",
            "1.1 \t が \t 新興\n",
            "1.2 \t イギリス \t 企業\n",
            "1.2 \t の \t を\n",
            "1.2 \t 新興 \t １０億\n",
            "1.1 \t 企業 \t ドル\n",
            "1.0 \t を \t で\n",
            "0.8 \t 10 \t 購入\n",
            "0.5 \t 億 \t を\n",
            "0.30000000000000004 \t ドル \t 検討\n",
            "0.0 \t で購入 \t None\n",
            "-0.2 \t を検討 \t None\n",
            "-0.39999999999999997 \t </s> \t None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "align(parsed_text, tokenized_text, scores)"
      ],
      "metadata": {
        "id": "K9rWWceL1R4G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46ab89a7-0679-42c4-a570-773500f1d597"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['アップル', 'が', 'イギリス', 'の', '新興', '企業', 'を', '億', 'ドル', 'で', '購入', 'を', '検討'],\n",
              " [0.7499999999999999,\n",
              "  1.1,\n",
              "  1.2,\n",
              "  1.2,\n",
              "  1.2,\n",
              "  1.1,\n",
              "  1.0,\n",
              "  0.5,\n",
              "  0.30000000000000004,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  -0.2,\n",
              "  -0.2])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing with more examples\n",
        "\n",
        "I'm testing with Chinese, Japanese, Korean, and Thai."
      ],
      "metadata": {
        "id": "ekrIeThhHwWS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(\"zh_core_web_md\")\n",
        "for doc in zh_sentences:\n",
        "    parsed_text = [i.text for i in nlp(doc)]\n",
        "    tokenized_text = [tokenizer.decode(i) for i in tokenizer(doc).input_ids]\n",
        "    scores = [0.2+np.round(np.sin(0.8*i/np.pi), decimals=1) for i in range(len(tokenized_text))]\n",
        "    print(align(parsed_text, tokenized_text, scores))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kW2eITFRGUHq",
        "outputId": "4caec5e7-2ae1-40c1-9443-cf622a7d0da7"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(['作为', '语言', '而言', '为', '世界', '使用', '人数', '最多', '的', '语言', '目前', '世界', '有', '五分之一', '人口', '做为', '母语'], [0.7, 0.8999999999999999, 1.1, 1.2, 1.2, 1.1, 1.0, 0.8, 0.8, 0.5, 0.0, -0.2, -0.39999999999999997, -0.7250000000000001, -0.8, -0.65, -0.3])\n",
            "(['汉语', '有', '多种', '分支', '当中', '官话', '最为', '流行', '为', '中华', '人民', '共和国', '的', '国家', '通用', '语言', '又', '称为', '普通话', '以及', '中华', '民国', '的', '国语'], [0.7, 0.8999999999999999, 1.1, 1.2, 1.1, 0.9, 0.5, 0.30000000000000004, -0.2, -0.39999999999999997, -0.39999999999999997, -0.39999999999999997, -0.6000000000000001, -0.6000000000000001, -0.7, -0.8, -0.7, -0.6000000000000001, -0.3333333333333333, 0.5, 0.8, 1.0, 1.2, 1.2])\n",
            "(['此外', '中文', '还是', '联合国', '正式', '语文', '并', '被', '上海', '合作', '组织', '等', '国际', '组织', '采用', '为', '官方', '语言'], [0.5, 0.8999999999999999, 1.1, 1.2, 1.2, 1.15, 0.8, 0.5, 0.30000000000000004, 0.0, -0.2, -0.39999999999999997, -0.6000000000000001, -0.7, -0.8, -0.8, -0.7, -0.6000000000000001])\n",
            "(['在', '中国', '大陆', '汉语', '通称', '为', '汉语'], [0.7, 0.7, 0.8999999999999999, 1.2, 1.2, 1.2, 1.0])\n",
            "(['在', '联合国', '台湾', '香港', '及', '澳门', '通称', '为', '中文'], [0.5, 0.6999999999999998, 1.1, 1.2, 1.2, 1.1, 0.65, 0.5, 0.0])\n",
            "(['在', '新加坡', '及', '马来西亚', '通称', '为', '华语'], [0.5, 0.6999999999999998, 0.8999999999999999, 1.1, 1.2, 1.2, 0.9])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(\"ja_core_news_md\")\n",
        "for doc in ja_sentences:\n",
        "    parsed_text = [i.text for i in nlp(doc)]\n",
        "    tokenized_text = [tokenizer.decode(i) for i in tokenizer(doc).input_ids]\n",
        "    scores = [0.2+np.round(np.sin(0.8*i/np.pi), decimals=1) for i in range(len(tokenized_text))]\n",
        "    print(align(parsed_text, tokenized_text, scores))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ey2qjTXhJkyf",
        "outputId": "14c07642-78c6-4e35-a723-a9ebfede02fc"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(['アップル', 'が', 'イギリス', 'の', '新興', '企業', 'を', '億', 'ドル', 'で', '購入', 'を', '検討'], [0.7499999999999999, 1.1, 1.2, 1.2, 1.2, 1.1, 1.0, 0.5, 0.30000000000000004, 0.0, 0.0, -0.2, -0.2])\n",
            "(['自動', '運転', '車', 'の', '損害', '賠償', '責任', '自動車', 'メーカー', 'に', '一定', 'の', '負担', 'を', '求める'], [0.7, 0.8999999999999999, 1.1, 1.2, 1.2, 1.2, 1.1, 0.8000000000000002, 0.5, 0.30000000000000004, 0.0, -0.2, -0.39999999999999997, -0.6000000000000001, -0.6000000000000001])\n",
            "(['歩道', 'を', '走る', '自動', '配達', 'ロボ', 'サンフランシスコ', '市', 'が', '走行', '禁止', 'を', '検討'], [0.7999999999999999, 1.1, 1.2, 1.2, 1.05, 0.65, -0.3875, -0.8, -0.7, -0.5, -0.2, 0.0, 0.0])\n",
            "(['ロンドン', 'は', 'イギリス', 'の', '大', '都市', 'です'], [0.85, 1.2, 1.2, 1.2, 1.1, 1.0, 0.8])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from spacy.lang.ko.examples import sentences as ko_sentences\n",
        "nlp = spacy.load(\"ko_core_news_sm\")\n",
        "for doc in ko_sentences:\n",
        "    parsed_text = [i.text for i in nlp(doc)]\n",
        "    tokenized_text = [tokenizer.decode(i) for i in tokenizer(doc).input_ids]\n",
        "    scores = [0.2+np.round(np.sin(0.8*i/np.pi), decimals=1) for i in range(len(tokenized_text))]\n",
        "    print(align(parsed_text, tokenized_text, scores))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HeBOCLctKBMe",
        "outputId": "db87aec8-f71b-42a4-f3cd-819fc841e3f0"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(['애플이', '영국의', '스타트업을', '억', '달러에', '인수하는', '것을', '알아보고', '있다'], [0.5666666666666667, 0.9666666666666667, 1.2, 1.1, 0.7000000000000001, -0.024999999999999994, -0.39999999999999997, -0.625, -0.8])\n",
            "(['자율주행', '자동차의', '손해', '배상', '책임이', '제조', '업체로', '옮겨', '가다'], [0.7999999999999999, 1.2, 1.15, 0.9, 0.43333333333333335, 0.0, -0.26666666666666666, -0.75, -0.75])\n",
            "(['샌프란시스코', '시가', '자동', '배달', '로봇의', '보도', '주행', '금지를', '검토', '중이라고', '합니다'], [1.05, 1.15, 1.0, 0.65, 0.20000000000000004, -0.2, -0.5, -0.7333333333333334, -0.8, -0.625, -0.39999999999999997])\n",
            "(['런던은', '영국의', '수도이자', '가장', '큰', '도시입니다'], [0.9, 1.2, 1.15, 1.0, 0.8, 0.38])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "th_sentences = [\"ช่วยพูดช้าๆได้ไหม\", \"ช่วยอธิบายให้ฉันฟังหน่อย\", \"ช่วยยกตัวอย่างให้หน่อย\"]\n",
        "import spacy_thai\n",
        "nlp=spacy_thai.load()\n",
        "for doc in th_sentences:\n",
        "    parsed_text = [i.text for i in nlp(doc)]\n",
        "    tokenized_text = [tokenizer.decode(i) for i in tokenizer(doc).input_ids]\n",
        "    scores = [0.2+np.round(np.sin(0.8*i/np.pi), decimals=1) for i in range(len(tokenized_text))]\n",
        "    print(align(parsed_text, tokenized_text, scores))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-iMznIfULmzV",
        "outputId": "b5f87001-5307-44e0-8673-39d052d63af2"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(['ชวย', 'พด', 'ชาๆ', 'ได', 'ไหม'], [0.5, 0.7, 0.9666666666666667, 1.2, 1.2])\n",
            "(['ชวย', 'อธบาย', 'ให', 'ฉน', 'ฟง', 'หนอย'], [0.5, 0.7, 0.8999999999999999, 1.1, 1.2, 1.2])\n",
            "(['ชวย', 'ยกตวอยาง', 'ให', 'หนอย'], [0.5, 0.8499999999999999, 1.1, 1.2])\n"
          ]
        }
      ]
    }
  ]
}
